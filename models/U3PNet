import torch
import torch.nn as nn
import torch.nn.functional as F


class U3PNet(nn.Module):
    def __init__(self, HSI_bands, MSI_bands, upscale, hidden_dim=64, bilinear=False):
        super(U3PNet, self).__init__()
        self.num_feature = hidden_dim
        self.hidden_dim = hidden_dim
        self.upscale = upscale
        ##############################

        self.unet = Unet(HSI_bands + MSI_bands, self.hidden_dim, bilinear)
        self.refine = nn.Sequential(
            nn.Conv2d(self.num_feature, self.hidden_dim, 5, 1, 2),
            nn.LeakyReLU(),
            nn.Conv2d(self.hidden_dim, HSI_bands, 3, 1, 1)
        )

    def forward(self, HSI, MSI):
        up_LRHSI = F.interpolate(HSI, scale_factor=self.upscale, mode="bicubic")  # 插值上采样
        up_LRHSI = up_LRHSI.clamp_(0, 1)
        data = torch.cat((up_LRHSI, MSI), 1)  
        high_pass = self.unet(data)
        high_pass = self.refine(high_pass)
        output = high_pass + up_LRHSI
        output = output.clamp_(0, 1)
        return output


class Unet(nn.Module):
    def __init__(self, in_channels, hidden_dim, bilinear=False):
        super(Unet, self).__init__()
        self.hidden_dim = hidden_dim
        self.prompt_len = 12
        self.increase = nn.Sequential(
            nn.Conv2d(in_channels, self.hidden_dim, 3, 1, 1),
            nn.LeakyReLU(),
            nn.Conv2d(self.hidden_dim, self.hidden_dim, 5, 1, 2)
        )
        
        self.down1 = (Down(self.hidden_dim, self.hidden_dim * 2))
        self.down2 = (Down(self.hidden_dim * 2, self.hidden_dim * 4))
        factor = 2 if bilinear else 1
        self.down3 = (Down(self.hidden_dim * 4, self.hidden_dim * 8 // factor))
        self.up1 = (Up(self.hidden_dim * 8, self.hidden_dim * 4 // factor, bilinear))
        self.up2 = (Up(self.hidden_dim * 4, self.hidden_dim * 2 // factor, bilinear))
        self.up3 = (Up(self.hidden_dim * 2, self.hidden_dim, bilinear))

        self.x1_expand = ResizeExpand(self.hidden_dim, self.hidden_dim * 2)
        self.x2_expand = ResizeExpand(self.hidden_dim * 2, self.hidden_dim * 4)
        self.x3_expand = ResizeExpand(self.hidden_dim * 4, self.hidden_dim * 8)
        
        self.prompt1 = PromptBlock(hidden_dim=self.hidden_dim, prompt_len=self.prompt_len, prompt_size=1, out_channels=self.hidden_dim)
        self.prompt2 = PromptBlock(hidden_dim=self.hidden_dim*2, prompt_len=self.prompt_len, prompt_size=1, out_channels=self.hidden_dim*2)
        self.prompt3 = PromptBlock(hidden_dim=self.hidden_dim*4, prompt_len=self.prompt_len, prompt_size=1, out_channels=self.hidden_dim*4)
        self.prompt4 = PromptBlock(hidden_dim=self.hidden_dim*8, prompt_len=self.prompt_len, prompt_size=1, out_channels=self.hidden_dim*8)
        
        self.spatio_attention1 = SALayer(self.hidden_dim)
        self.spatio_attention2 = SALayer(self.hidden_dim * 2)
        self.spatio_attention3 = SALayer(self.hidden_dim * 4)
        self.channel_attention1 = CALayer(self.hidden_dim, 16, bias=False)
        self.channel_attention2 = CALayer(self.hidden_dim * 2, 16, bias=False)
        self.channel_attention3 = CALayer(self.hidden_dim * 4, 16, bias=False)
        self.res_block1 = ResBlock(self.hidden_dim, self.hidden_dim)
        self.res_block2 = ResBlock(self.hidden_dim * 2, self.hidden_dim * 2)
        self.res_block3 = ResBlock(self.hidden_dim * 4, self.hidden_dim * 4)
        self.relu1 = nn.LeakyReLU()
        self.relu2 = nn.LeakyReLU()


    def forward(self, x):
        x1 = self.increase(x)
        x2 = self.down1(x1) + self.x1_expand(x1) * nn.Parameter(torch.zeros(1).to(x1.device))
        x3 = self.down2(x2) + self.x2_expand(x2) * nn.Parameter(torch.zeros(1).to(x2.device))
        x4 = self.down3(x3) + self.x3_expand(x3) * nn.Parameter(torch.zeros(1).to(x3.device))

        y = x4 * self.prompt4(x4) 
        prompt1 = self.prompt1(x1)
        prompt2 = self.prompt2(x2)
        prompt3 = self.prompt3(x3)

        y3_sa = self.relu1(self.spatio_attention3(self.res_block3(x3))) * prompt3
        y3_ca = self.relu2(self.channel_attention3(self.res_block3(x3))) * prompt3

        y2_sa = self.relu1(self.spatio_attention2(self.res_block2(x2))) * prompt2
        y2_ca = self.relu2(self.channel_attention2(self.res_block2(x2))) * prompt2

        y1_sa = self.relu1(self.spatio_attention1(self.res_block1(x1))) * prompt1
        y1_ca = self.relu2(self.channel_attention1(self.res_block1(x1))) * prompt1

        x = self.up1(x4 + y, (y3_sa + y3_ca + self.res_block3(x3) * prompt3).clamp_(0, 1))
        x = self.up2(x, (y2_sa + y2_ca + self.res_block2(x2) * prompt2).clamp_(0, 1))
        x = self.up3(x, (y1_sa + y1_ca + self.res_block1(x1) * prompt1).clamp_(0, 1))
        return x


class PromptBlock(nn.Module):
    def __init__(self, hidden_dim=64, prompt_len=5, prompt_size=1, out_channels=64):
        super(PromptBlock, self).__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.prompt_param = nn.Parameter(torch.randn(1, prompt_len, hidden_dim, prompt_size, prompt_size))
        self.mlp = MLP(out_channels, prompt_len)
        self.conv3x3 = nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1)

    def forward(self, x):
        B, C, H, W = x.shape
        shortcut = x
        emb = self.pool(x)  # Shape: [B, C, 1, 1]
        prompt_weights = F.softmax(self.mlp(emb), dim=1).squeeze(-1).squeeze(-1)  # Shape: [B, prompt_len]
        
        prompt = torch.einsum('b l,p d h w -> b p d h w', prompt_weights, self.prompt_param.squeeze(0))
        prompt = torch.sum(prompt, dim=1)  # Shape: [B, hidden_dim, prompt_size, prompt_size]
        prompt = F.interpolate(prompt, (H, W), mode="bilinear")  # Shape: [B, hidden_dim, H, W]
        prompt = F.leaky_relu(self.conv3x3(prompt)) + shortcut  # Shape: [B, hidden_dim, H, W]
        return prompt


class DepthwiseConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DepthwiseConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        self.depth_conv = nn.Conv2d(self.in_channels, self.in_channels, kernel_size=3, padding=1, groups=self.in_channels)
        self.point_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1)

    def forward(self, x):
        x = self.depth_conv(x)
        x = self.point_conv(x)
        return x


class DoubleConv(nn.Module):  # 使用深度可分离卷积
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super(DoubleConv, self).__init__()
        if not mid_channels:
            mid_channels = out_channels

        self.double_conv = nn.Sequential(
            DepthwiseConv(in_channels, mid_channels),  # 第一个卷积层使用DW卷积
            nn.LeakyReLU(),
            DepthwiseConv(mid_channels, out_channels),  # 第二个卷积层使用DW卷积
            nn.LeakyReLU()
        )

    def forward(self, x):
        return self.double_conv(x)


class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResBlock, self).__init__()
        self.conv1 = DoubleConv(in_channels, out_channels)
        self.relu1 = nn.LeakyReLU()
        self.conv2 = DoubleConv(in_channels, out_channels)
        self.relu2 = nn.LeakyReLU()

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.conv2(out)
        out = self.relu2(out)
        out += residual
        return out


class Down(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.maxpool_conv = nn.MaxPool2d(2)
        self.conv2 = nn.Sequential(
            DoubleConv(in_channels, out_channels),
            nn.LeakyReLU()
        ) 

    def forward(self, x):
        x = self.maxpool_conv(x)
        x = self.conv2(x)
        return x


class Up(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=False):
        super().__init__()
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2, padding=0)
            
        self.conv2 = nn.Sequential(
            DoubleConv(in_channels, out_channels),
            nn.LeakyReLU()
        )         

    def forward(self, x1, x2):
        x1 = self.up(x1)
        x = torch.cat((x2, x1), 1)
        x = self.conv2(x)
        return x


class ResizeExpand(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1)

    def forward(self, x):
        x = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)
        x = self.conv(x)
        return x


class MainExtractionBlock(nn.Module):
    def __init__(self, in_channels, out_channels) -> None:
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels,3,1,1)
        self.conv2 = nn.Conv2d(in_channels, out_channels,3,1,1)
        
    def forward(self,x):
        x = F.relu(self.conv1(x))
        x = F.leaky_relu(self.conv2(x))
        return x


class MLP(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_dim=128, drop_ratio=0.3):
        super(MLP, self).__init__()
        self.fc1 = nn.Conv2d(in_channels, hidden_dim, 1)
        self.dropout = nn.Dropout(drop_ratio) if drop_ratio > 0 else nn.Identity()
        self.fc2 = nn.Conv2d(hidden_dim, out_channels, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.leaky_relu(self.fc2(x))
        return x


class CALayer(nn.Module):
    def __init__(self, channel, reduction=16, groups=8, bias=False):
        super(CALayer, self).__init__()
        self.groups = groups
        self.mlp = MLP(channel, channel, channel // reduction)
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv_ca = nn.Sequential(
            nn.Conv2d(channel, channel // reduction, 1, padding=0),
            nn.LeakyReLU(),
            nn.Conv2d(channel // reduction, channel, 1, padding=0),
            nn.Sigmoid()
        )

    def channel_shuffle(self, x, groups):
        b, c, h, w = x.size()
        channels_per_group = c // groups
        x = x.view(b, groups, channels_per_group, h, w)
        x = x.permute(0, 2, 1, 3, 4).contiguous()
        x = x.view(b, c, h, w)
        return x

    def forward(self, x):
        x = self.channel_shuffle(x, self.groups)
        x = self.mlp(x)
        y = self.avg_pool(x)
        y = self.conv_ca(y)
        return x * y
    
    
class SALayer(nn.Module):
    def __init__(self, channel, reduction=8, bias=False):
        super(SALayer, self).__init__()
        self.conv_du = nn.Sequential(
            nn.Conv2d(channel, channel // reduction, 1),
            nn.LeakyReLU(),
            nn.Conv2d(channel // reduction, channel // reduction, 5, padding=2, stride=1),
            nn.LeakyReLU(),
            nn.Conv2d(channel // reduction, channel, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        y = self.conv_du(x)
        return x * y
